---
title: "Time series analysis"
author: "Sivert Selnes"
date: "sept"
output: 
  html_document:
    toc: true
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("itsmr")
```


```{r, echo=F}
#setwd("~/Tidsrekker/TMA4285_Project_1")
dataseries <- read.table("DataEx3G14.txt")
```


### Abstract

### Introduction


### Theory
#### What is a time series and an ARMA-process.
 
A time series is a sequence of observations with timestamps. If there is some random noise involved in the data, we might represent the time series as an ARMA model. That means that the data value at a given time $t$ is a linear combination of the last $p$ data realizations added to another linear combination of the last $q$ white-noise terms $Z$. An ARMA(2,1)-process is therefore a model that includes the last two datapoints and the last noise-term. Using the backshift operator $B$, this is expressed as
 \[
 \phi(B)X_t =\theta(B)Z_t, \\
 BX_t = X_{t-1}
 \]
 where the polynomials $\phi$ and $\theta$ in this case is limited to
 \[
 \phi(B) = 1-\phi_1B - \phi_2B, \\
 \theta(B) = 1+\theta_1B. \\
 \]
 Then the ARMA(2,1) process is represented by the equation
 \[
 X_t =\phi_1X_{t-1} - \phi_2X_{t-2} + Z_t + \theta_1Z_{t-1}.
 \]
 
#### The autocorrelation function
To determine the best order of ARMA(p,q) for representing the time series we want to plot the sample autocorrelation function (ACF), which is computed from the autocovariance function. The ACVF is defined as

\[
\gamma(h) = Cov(X_{t+h}-X_{t}) = E(X_{t+h}-\mu)(X_{t}-\mu.)
\]
and the sample ACVF becomes
\[
\hat{\gamma}(h)=n^{-1}\sum_{t=1}^{n-|h|}(x_{t+h}-\bar{x})(x_{t}-\bar{x}), \quad -n<h<n.
\]
Note that for this particular time series sample it is assumed to be an ARMA(2,1), hence the true mean is (assumed) $\mu=0$.

Finally we obtain the ACF by normalization of the ACVF: $\hat{\rho}(h) = \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}$.

The plot of the ACF $\hat{\gamma}(h)$ will reveal the time series' correlation at different lags $h$, thus making it suitable for determing the appropriate number of terms in the moving-average part of the model. An MA($q$) is dependent on white noise terms up to and including lag $h=q$. We expect to see a sharp decline in correlation at lags greater than this "memory level" $q$, since the sample value is no longer dependent on the previous noise.


#### The partial autocorrelation function
The partial autocorrelation function is the correlation function with the linear effects of $\{x_{t+1},x_{t+2},...,x_{t-1+h}\}$ are removed. That means that the only contribution to the correlation between two values at lag $h$ is the direct correlation between the two, and not the implicit correlation from one $x$ to the next, and so on. For example will an AR(1)-process have implications for realizations far beyond one time step, but removing this recursive feature we can use the PACF for deciding on the appropriate order $p$. 
\[
\alpha(1)=Corr(X_{t+1},X_t), \\
\alpha(h) = Corr(X_{t+h}-P_{t,h}(X_{t+h}),X_{t}-P_{t,h}(X_{t}))
\]

where $P_{t,h}X_t$ is the best linear prediction of $X_{t+h}$ given $\{x_{t+1},x_{t+2},...,x_{t-1+h}\}$.

#### Model parameters $\phi$, $\theta$ and $\sigma^2$
 For estimation of the model parameters we use the maximum likelihood function. We assume that the model order $(2,1)$ is known, and that the data has zero mean. This results in the ARMA model 
 
\[
 X_t =\phi_1X_{t-1} - \phi_2X_{t-2} + Z_t + \theta_1Z_{t-1}.
\]
The $\{X_t\}$ is gaussian distributed since the white noise included is assumed to be gaussian, independently distributed, $\{Z_t\} \sim N(0,\sigma^2)$. The approach for estimating to parameters is then to maximize a likelihood function of a gaussian distribution. This optimization for determining the best parameters is done numerically. The likelihood function is the familiar multivariable normal distribution, 

\[
L(\phi, \theta, \sigma^2) = f_{\phi, \theta, \sigma^2}(X_1,...,X_n) \\
= \frac{1}{(2\pi)^{n/2}|\Gamma_n|^{1/2}}\exp{(-\frac{1}{2}X'\Gamma_n^{-1}X)},
\]
where the $\Gamma$ is the covariance matrix of the series $\{X\}$. 


#### Model prediction at sample points
A convenient way to forecasting an ARMA(p,q) is to use the innovations algorithm, which recursively calculate each forecast ("innovation").

\[
 X_{n+1}^n =
 \begin{cases}
 \sum_{j=1}^n \theta_{nj}(X_{n+1-j}-X_{n+1-j}^{n-j}) & n <p, \\
 \sum_{j=1}^p \phi_jX_{n+1-j}+\sum_{j=1}^q \theta_{nj}(X_{n+1-j}-X_{n+1-j}^{n-j}) & n\geq p.
 \end{cases}
 \]
 

#### Model diagnostics: strategies for assessing model choice
When the model parameters $(p,q)$ is chosen for the ARMA-process, the residuals should ideally only consist of white noise (otherwise the model choice does not cover the underlying causes). Therefore we can apply the ACF and the PACF to the residuals, rejecting the model if the correlations exceed the standard deviation more often than expected (for example 5 $\%$ of the time for a 95 $\%$ c.i.).
 
### Data analysis
 * The plots from 1-4 go in here, basicly most of the results of the R-code are displayed here.


### Discussion
 * Comments on prelim plots: time series, ACF, PACF
 
 * Comments on estimated paramateres and uncertainties in particular
 
 * Comments on fit/prediction of the chosen ARMA model
 
 * Finally, comment on the diagnostics, AICC
 
 
### Conclusion

<!--
What can be assumed?
- Causal time series (Mentioned in the lecture, Monday 17th Sep)

If we make other assumptions, write that down. 

-->




### Appendix, References

