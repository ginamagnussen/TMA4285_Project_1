---
title: "Time series analysis"
author: "Sivert Selnes"
date: "sept"
output: 
  html_document:
    toc: true
    toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("itsmr")
```


```{r, echo=F}
#setwd("~/Tidsrekker/TMA4285_Project_1")
dataseries <- read.table("DataEx3G14.txt")
```


### Abstract

### Introduction


### Theory
#### What is a time series and an ARMA-process.
 
A time series is a sequence of observations with timestamps. If there is some random noise involved in the data, we might represent the time series as an ARMA model. That means that the data value at a given time $t$ is a linear combination of the last $p$ data realizations added to another linear combination of the last $q$ white-noise terms $Z$. An ARMA(2,1)-process is therefore a model that includes the last two datapoints and the last noise-term. Using the backshift operator $B$, this is expressed as
 \[
 \phi(B)X_t =\theta(B)Z_t, \\
 BX_t = X_{t-1}
 \]
 where the polynomials $\phi$ and $\theta$ in this case is limited to
 \[
 \phi(B) = 1-\phi_1B - \phi_2B, \\
 \theta(B) = 1+\theta_1B. \\
 \]
 Then the ARMA(2,1) process is represented by the equation
 \[
 X_t =\phi_1X_{t-1} - \phi_2X_{t-2} + Z_t + \theta_1Z_{t-1}.
 \]
 
#### Calculation of ACF
After establishing the ARMA-model, we want to plot the sample autocorrelation function (ACF). This will reveal if the data is likely to origin from an ARMA(2,1), or if behaves significantly different from the expected ACF of an ARMA(2,1). The sample autocovariance function (ACVF) is defined as

\[
\hat{\gamma}(h) = n^{-1}\sum_{t=1}^{n-|h|}(X_{t+|h|}-\bar{X}_n)(X_{t}-\bar{X}_n),
\]
and we obtain the ACF by normalizing: $\hat{\rho}(h) = \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}$.

#### Calculation of PACF
The partial autocorrelation function is the correlation function with lags shorter than $h$ removed. Since the CF is corrected/controlled for the shorter lags up to $h$, it is suited to estimate the appropriate order $p$ for the autoregressive part. The PACF is defined as

\[
\alpha(1)=Corr(X_{t+1},X_t), \\
\alpha(h) = Corr(X_{t+h}-P_{t,h}(X_{t+h}),X_{t}-P_{t,h}(X_{t}))
\]

where $P_{t,h}X_t$ is the \todo{Fyll inn her}

#### Calculation of model parameters $\phi$, $\theta$ and $\sigma^2$
 * Need the uncertainty in those as well
 
#### Model prediction at sample points
 A convenient way to produce model prediction values is to use the innovations algorithm, which simiplifies in the case of a casual ARMA-process. 
 
 Innovations algorithm:
 
 \[
 v_0 = \kappa(1,1), \\
 \theta_{n,n-k} = v_k^{-1}(\kappa(n+1,k+1)-\sum_{j=0}^{k-1}\theta_{k,k-j}\theta_{n,n-j}v_j, \hspace{5mm}  0 \leq k < n \\
 \]
 
 \[
 \hat{X} = 
 \begin{cases}
 \sum_{j=1}^n \theta_{nj}(X_{n+1-j}-\hat{X}_{n+1-j}) & 1 \leq n < m, \\
 \phi_1X_n + ... +\phi_pX_{n+1-p} + \sum_{j=1}^p \theta_{nj}(X_{n+1-j}-\hat{X}_{n+1-j}) & n \geq m.
 \end{cases}
 \]
 
 * Calculate prediction for future values
 * Calculate uncertainty in both cases
 
 * Diagnostics: strategies for assessing model choice (AICC)
 
### Data analysis
 * The plots from 1-4 go in here, basicly most of the results of the R-code are displayed here.


### Discussion
 * Comments on prelim plots: time series, ACF, PACF
 
 * Comments on estimated paramateres and uncertainties in particular
 
 * Comments on fit/prediction of the chosen ARMA model
 
 * Finally, comment on the diagnostics, AICC
 
 
### Conclusion

<!--
What can be assumed?
- Causal time series (Mentioned in the lecture, Monday 17th Sep)

If we make other assumptions, write that down. 

-->




### Appendix, References

