---
title: "Analysis of an ARMA(2,1)"
author: "Sivert Selnes, Gina Magnussen, Kristine Lund Mathisen"
date: "23. september 2018"
output:
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("itsmr") # Time series analysis using the Innovations Algorithm
library("TTR") # Functions to create Technical Trading Rules
library("aTSA")
library("tseries") # Time series analysis and computational finance
library("forecast")
library("ggplot2")
```


```{r, echo=F}
#setwd("~/Tidsrekker/TMA4285_Project_1")
dataseries <- ts(read.table("DataEx3G14.txt"))
```


# Abstract

# Introduction


# Theory
## What is a time series and an ARMA-process.
 
A time series is a sequence of observations with timestamps. If there is some random noise involved in the data, we might represent the time series as an ARMA model. That means that the data value at a given time $t$ is a linear combination of the last $p$ data realizations added to another linear combination of the last $q$ white-noise terms $Z$. An ARMA(2,1)-process is therefore a model that includes the last two datapoints and the last noise-term. Using the backshift operator $B$, this is expressed as
 \[
 \phi(B)X_t =\theta(B)Z_t, \\
 BX_t = X_{t-1}
 \]
 where the polynomials $\phi$ and $\theta$ in this case is limited to
 \[
 \phi(B) = 1-\phi_1B - \phi_2B, \\
 \theta(B) = 1+\theta_1B. \\
 \]
 Then the ARMA(2,1) process is represented by the equation
 \[
 X_t =\phi_1X_{t-1} - \phi_2X_{t-2} + Z_t + \theta_1Z_{t-1}.
 \]
 
## The autocorrelation function
To determine the best order of ARMA(p,q) for representing the time series we want to plot the sample autocorrelation function (ACF), which is computed from the autocovariance function. The ACVF is defined as

\[
\gamma(h) = Cov(X_{t+h}-X_{t}) = E(X_{t+h}-\mu)(X_{t}-\mu.)
\]
and the sample ACVF becomes
\[
\hat{\gamma}(h)=n^{-1}\sum_{t=1}^{n-|h|}(x_{t+h}-\bar{x})(x_{t}-\bar{x}), \quad -n<h<n.
\]
Note that for this particular time series sample it is assumed to be an ARMA(2,1), hence the true mean is (assumed) $\mu=0$.

Finally we obtain the ACF by normalization of the ACVF: $\hat{\rho}(h) = \frac{\hat{\gamma}(h)}{\hat{\gamma}(0)}$.

The plot of the ACF $\hat{\gamma}(h)$ will reveal the time series' correlation at different lags $h$, thus making it suitable for determing the appropriate number of terms in the moving-average part of the model. An MA($q$) is dependent on white noise terms up to and including lag $h=q$. We expect to see a sharp decline in correlation at lags greater than this "memory level" $q$, since the sample value is no longer dependent on the previous noise.


## The partial autocorrelation function
The partial autocorrelation function is the correlation function with the linear effects of $\{x_{t+1},x_{t+2},...,x_{t-1+h}\}$ are removed. That means that the only contribution to the correlation between two values at lag $h$ is the direct correlation between the two, and not the implicit correlation from one $x$ to the next, and so on. For example will an AR(1)-process have implications for realizations far beyond one time step, but removing this recursive feature we can use the PACF for deciding on the appropriate order $p$. 
\[
\alpha(1)=Corr(X_{t+1},X_t), \\
\alpha(h) = Corr(X_{t+h}-P_{t,h}(X_{t+h}),X_{t}-P_{t,h}(X_{t}))
\]

where $P_{t,h}X_t$ is the best linear prediction of $X_{t+h}$ given $\{x_{t+1},x_{t+2},...,x_{t-1+h}\}$.

## Model parameters $\phi$, $\theta$ and $\sigma^2$
 For estimation of the model parameters we use the maximum likelihood function. We assume that the model order $(2,1)$ is known, and that the data has zero mean. This results in the ARMA model 
 
\[
 X_t =\phi_1X_{t-1} - \phi_2X_{t-2} + Z_t + \theta_1Z_{t-1}.
\]
The $\{X_t\}$ is gaussian distributed since the white noise included is assumed to be gaussian, independently distributed, $\{Z_t\} \sim N(0,\sigma^2)$. The approach for estimating to parameters is then to maximize a likelihood function of a gaussian distribution. This optimization for determining the best parameters is done numerically. The likelihood function is the familiar multivariable normal distribution, 

\[
L(\phi, \theta, \sigma^2) = f_{\phi, \theta, \sigma^2}(X_1,...,X_n) \\
= \frac{1}{(2\pi)^{n/2}|\Gamma_n|^{1/2}}\exp{(-\frac{1}{2}X'\Gamma_n^{-1}X)},
\]
where the $\Gamma$ is the covariance matrix of the series $\{X\}$. 


## Model prediction at sample points
A convenient way to forecasting an ARMA(p,q) is to use the innovations algorithm, which recursively calculates each forecast ("innovation").

\[
 X_{n+1}^n =
 \begin{cases}
 \sum_{j=1}^n \theta_{nj}(X_{n+1-j}-X_{n+1-j}^{n-j}) & n <p, \\
 \sum_{j=1}^p \phi_jX_{n+1-j}+\sum_{j=1}^q \theta_{nj}(X_{n+1-j}-X_{n+1-j}^{n-j}) & n\geq p.
 \end{cases}
 \]
 

## Model diagnostics: strategies for assessing model choice
When the model parameters $(p,q)$ are chosen for the ARMA-process, the residuals should ideally only consist of white noise (otherwise the model choice does not cover the underlying causes). Therefore we can apply the ACF and the PACF to the residuals, rejecting the model if the correlations exceed the standard deviation more often than expected (for example 5 $\%$ of the time for a 95 $\%$ c.i.).

# Data analysis

## Plot of time series, ACF, PACF
```{r}
plot.ts(dataseries) # Plot of the time series itself
acf(dataseries) # Autocorrelation function
Pacf(dataseries) # Partial ACF
```
 
## Check for stationarity
```{r, warning=FALSE, message=FALSE}
 #Check stationarity
adf.test(dataseries) # Augmented Dickey-Fuller test
```

## Model fitting
```{r}
# Estimate ARMA model coefficients using maximum likelihood
arimaFit <- arima(dataseries, order = c(2,0,1), method = "ML")
arimaFit
res_scaled <- arimaFit$residuals/sqrt(arimaFit$sigma2)
plot(res_scaled, main ="Rescaled residuals")
hist(res_scaled, main ="Histogram of rescaled residuals")
```

## Analysis of the residuals

```{r}
acf(res_scaled) # To find order of MA(q) (lags)
pacf(res_scaled) # To find order of AR(p)

vcov(arimaFit)

```

## Prediction for sample values
```{r}
# QQplots
qqnorm(res_scaled, main = "Q-Q Plot: ARMA model"); qqline(res_scaled) 

# Predicted values of the model for the last 100 samples of the dataseries
Xhat <- list()
phi1 <- arimaFit$coef[1]; phi2 <- arimaFit$coef[2]; theta1 <- arimaFit$coef[3]; res <- arimaFit$residuals
for (t in 3:500){
  Xhat[t] = phi1*dataseries[t-1] + phi2*dataseries[t-2] + theta1*res[t-1] + res[t]
}
plot(401:500, dataseries[401:500], "l", main = "Time series and predicted values", xlab = "Time step", ylab = "Value")
lines(401:500, Xhat[401:500], col = "red")



# Forecast
##################################################################
fcast = forecast(arimaFit, h=20)
pred <- predict(arimaFit, n.ahead = 20, se.fit = TRUE)
plot(fcast)



```

## AICc
```{r, warning = FALSE, message=FALSE}
# AIC
arimaFit$aic

# Plots of AIC
# Which order p,q is best, compare AIC
frame <- data.frame(p = 0, q = 0, aic = 0)
for(p in 1:5){
  for(q in 1:5){
    fit <- arima(dataseries, order = c(p,0,q), method = "ML")
    frame <- rbind(frame, c(p,q,fit$aic))
  }
}
frame <- frame[-1,]
ggplot(frame, aes(p,q)) + geom_raster(aes(fill = aic))


```


# Discussion
 * Comments on prelim plots: time series, ACF, PACF
 
 
The given time series is assumed to be without trend and seasonality. If we did not know this, the time series plot would suggest this as there is no apparent trend or repeating patterns visible.

ACF: There is most correlation at lag $h=1,2$ and $3$, and thus $q=3$ could be an appropriate choice for the moving average.

PACF: From the partial correlation plot, we see that lag $h=2$ could be a good choice for $p$. 

In addition, the augmented Dickey-Fuller test supports the assumption of a stationary time series.
 
 * Comments on estimated paramateres and uncertainties in particular
 
Estimated values and uncertainties for model coefficients $\phi, \theta$ and $\sigma^2$ can be found under "Model fitting" in "Data analysis". Note that due to the linearity of the assumed ARMA-model, the standard deviation of the intercept is the standard error for $\sigma^2$. The standard error for all parameters ar fairly small, indicating a close fit to the data.
 
 * Comments on fit/prediction of the chosen ARMA model
 
The Q-Q plot shows that the residuals seem to be normally distributed, and thus that the model is a good fit. 
Calculating the predicted series $\hat{X}_t$ for the last hundred time steps of the series and plotting it against the original series, we can see that the predictions are very accurate.

Forecasting from the model, it can be expected that the predicted future values will be more and more similar to the mean with time, since the next step is dependent on the previous one. This seems to be the case.

 
 
 * Finally, comment on the diagnostics, AICC
 
Further analysis of the fit of the ARMA(2,1)-model includes looking at the residuals. 
If the model is a good fit, the rescaled residuals should follow a white noise process with mean equal to zero and variance equal to one. 

Thus the graph of the rescaled residuals should resemble that of a WN(0,1) process. In addition, the histogram of the residuals is a rough check for the fit of the model. Both these plots support the choice of an AMRA(2,1)-model.

A more precise check is viewing the ACF and the PACF. Since correlations for more or less all lags $h$ do not exceed the standard deviation, the chosen model seems to be a good fit.

A low AIC value supports this further. For comparison, ARMA(p,q) models were fitted for $p=1,...,5$ and $q=1,...5$ and the corresponding AIC values were plotted in a raster plot. 
This plot also indicates that the fitted model is a good choice, since the ARMA(2,1) model has the lowest AIC value.
 
 
# Conclusion
The time series fulfilled the assumtion of stationarity, and an ARMA(p,q) could therefore be fitted to the data. The analysis of the time series suggested that the (2,1) model was a good fit to the data since the residuals showed no evidence of components that differed from white noise. The initial ACF and PACF plots suggest, however, an ARMA of order (2,3), since the correlation (and partial correlation) was significant up to $p=2$ and $q=3$. Finally, the AIC raster plot gave the lowest AIC for a model of order (2,1).


# Appendix, References

## Appendix
```{r, eval =FALSE}

# TMA4285 Time series models, autumn 2018

library("itsmr") # Time series analysis using the Innovations Algorithm
library("TTR") # Functions to create Technical Trading Rules
library("aTSA")
library("tseries") # Time series analysis and computational finance
library("forecast")
library("ggplot2")


# Import ARMA-series
dataseries <- ts(read.table("DataEx3G14.txt"))


# 1. Plotting of relevant statistics
###############################################

plot.ts(dataseries) # Plot of the time series itself

acf(dataseries) # Autocorrelation function

Pacf(dataseries) # Partial ACF

#Check stationarity
adf.test(dataseries) # Augmented Dickey-Fuller test
# p-value < 0.05 indicates the TS is stationary


# Estimate ARMA model coefficients using maximum likelihood
############################################################

## ARIMA
arimaFit <- arima(dataseries, order = c(2,0,1), method = "ML")
arimaFit
res_scaled <- arimaFit$residuals/sqrt(arimaFit$sigma2)
plot(res_scaled, main ="Rescaled residuals")
hist(res_scaled, main ="Histogram of rescaled residuals")

## Order selection

# To check if chosen model is good
acf(res_scaled) 
pacf(res_scaled) 

vcov(arimaFit)


# QQplots
qqnorm(res_scaled, main = "Q-Q Plot: ARMA model"); qqline(res_scaled) #Axis?

# Predicted values of the model for the last 100 samples of the dataseries
Xhat <- list()
phi1 <- arimaFit$coef[1]; phi2 <- arimaFit$coef[2]; theta1 <- arimaFit$coef[3]; res <- arimaFit$residuals
for (t in 3:500){
  Xhat[t] = phi1*dataseries[t-1] + phi2*dataseries[t-2] + theta1*res[t-1] + res[t]
}
plot(401:500, dataseries[401:500], "l", main = "Time series and predicted values", xlab = "Time step", ylab = "Value")
lines(401:500, Xhat[401:500], col = "red")



# Forecast
##################################################################
fcast = forecast(arimaFit, h=20)
pred <- predict(arimaFit, n.ahead = 20, se.fit = TRUE)
pplot(fcast)



# AIC
arimaFit$aic

# Plots of AIC
# Which order p,q is best, compare AIC
frame <- data.frame(p = 0, q = 0, aic = 0)
for(p in 1:5){
  for(q in 1:5){
    fit <- arima(dataseries, order = c(p,0,q), method = "ML")
    frame <- rbind(frame, c(p,q,fit$aic))
  }
}
frame <- frame[-1,]
ggplot(frame, aes(p,q)) + geom_raster(aes(fill = aic))

```



## Reference
Brockwell, Peter J., Davis, Richard A. 2002. $\textit{Introduction to time series and forecasting}$. 2nd ed. New York: Springer Science
